{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1083a8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1524cd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set -> (62658, 1) (62658, 3)\n",
      "Validation Set -> (20886, 1) (20886, 3)\n",
      "Test Set -> (20886, 1) (20886, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load data from bjp.csv into a DataFrame\n",
    "df = pd.read_csv('bjp.csv')\n",
    "X = df.iloc[:, 0].to_frame()\n",
    "y = pd.get_dummies(df.iloc[:, 1])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "print('Train Set ->', X_train.shape, y_train.shape)\n",
    "print('Validation Set ->', X_val.shape, y_val.shape)\n",
    "print('Test Set ->', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddf05a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def f1_score(precision, recall):\n",
    "    ''' Function to calculate f1 score '''\n",
    "    \n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ef7fc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 100, 32)           160000    \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 100, 32)           3104      \n",
      "                                                                 \n",
      " max_pooling1d_7 (MaxPoolin  (None, 50, 32)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " bidirectional_6 (Bidirecti  (None, 64)                16640     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 179939 (702.89 KB)\n",
      "Trainable params: 179939 (702.89 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "\n",
    "vocab_size = 5000\n",
    "embedding_size = 32\n",
    "max_len = 100  # You need to define max_len based on your data\n",
    "epochs = 10\n",
    "initial_learning_rate = 0.1\n",
    "decay_steps = 1000  # You may need to adjust this based on your dataset size\n",
    "momentum = 0.8\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True)\n",
    "\n",
    "# SGD optimizer with learning rate schedule\n",
    "sgd = SGD(learning_rate=lr_schedule, momentum=momentum, nesterov=False)\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_size, input_length=max_len))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile model with new optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d336e71f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Compiling & training\u001b[39;00m\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 13\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, epochs\u001b[38;5;241m=\u001b[39mepochs, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Evaluation on Test data\u001b[39;00m\n\u001b[1;32m     15\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mEagerTensor(value, ctx\u001b[38;5;241m.\u001b[39mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 100\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "# Building the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_size, input_length=max_len))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "# Compiling & training\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "# Evaluation on Test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "# Predictions on Test data\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_pred_classes = np.argmax(y_test, axis=1)\n",
    "# Calculate additional metrics\n",
    "f1 = f1_score(y_test, y_test_pred_classes, average='weighted')\n",
    "precision = precision_score(y_test, y_test_pred_classes, average='weighted')\n",
    "recall = recall_score(y_test, y_test_pred_classes, average='weighted')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2377ca53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train), type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ec0612e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "# Common hyperparameters\n",
    "vocab_size = 5000\n",
    "embedding_size = 32\n",
    "max_len = 100  # You need to define max_len based on your data\n",
    "epochs = 10\n",
    "initial_learning_rate = 0.1\n",
    "decay_steps = 1000  # You may need to adjust this based on your dataset size\n",
    "momentum = 0.8\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True)\n",
    "\n",
    "# SGD optimizer with learning rate schedule\n",
    "sgd = SGD(learning_rate=lr_schedule, momentum=momentum, nesterov=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b014bfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CNN model\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Embedding(vocab_size, embedding_size, input_length=max_len))\n",
    "cnn_model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "cnn_model.add(Bidirectional(LSTM(32)))\n",
    "cnn_model.add(Dropout(0.4))\n",
    "cnn_model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile CNN model with new optimizer\n",
    "cnn_model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a69ce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_12 (Embedding)    (None, 100, 32)           160000    \n",
      "                                                                 \n",
      " conv1d_9 (Conv1D)           (None, 100, 32)           3104      \n",
      "                                                                 \n",
      " max_pooling1d_9 (MaxPoolin  (None, 50, 32)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " bidirectional_8 (Bidirecti  (None, 64)                16640     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 179939 (702.89 KB)\n",
      "Trainable params: 179939 (702.89 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print CNN model summary\n",
    "cnn_model.summary()\n",
    "\n",
    "# Assuming X_train, y_train, X_test, y_test are defined previously\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b4f0994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_13 (Embedding)    (None, 100, 32)           160000    \n",
      "                                                                 \n",
      " conv1d_10 (Conv1D)          (None, 100, 32)           3104      \n",
      "                                                                 \n",
      " max_pooling1d_10 (MaxPooli  (None, 50, 32)            0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " bidirectional_9 (Bidirecti  (None, 64)                16640     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 179939 (702.89 KB)\n",
      "Trainable params: 179939 (702.89 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 45\u001b[0m\n\u001b[1;32m     40\u001b[0m cnn_model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Assuming X_train, y_train, X_test, y_test are defined previously\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Train CNN model\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m history_cnn \u001b[38;5;241m=\u001b[39m cnn_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39mepochs, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Evaluate CNN model on the test set\u001b[39;00m\n\u001b[1;32m     48\u001b[0m test_loss_cnn, test_accuracy_cnn \u001b[38;5;241m=\u001b[39m cnn_model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mEagerTensor(value, ctx\u001b[38;5;241m.\u001b[39mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "# Common hyperparameters\n",
    "vocab_size = 5000\n",
    "embedding_size = 32\n",
    "max_len = 100  # You need to define max_len based on your data\n",
    "epochs = 10\n",
    "initial_learning_rate = 0.1\n",
    "decay_steps = 1000  # You may need to adjust this based on your dataset size\n",
    "momentum = 0.8\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True)\n",
    "\n",
    "# SGD optimizer with learning rate schedule\n",
    "sgd = SGD(learning_rate=lr_schedule, momentum=momentum, nesterov=False)\n",
    "\n",
    "# Build CNN model\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Embedding(vocab_size, embedding_size, input_length=max_len))\n",
    "cnn_model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "cnn_model.add(Bidirectional(LSTM(32)))\n",
    "cnn_model.add(Dropout(0.4))\n",
    "cnn_model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile CNN model with new optimizer\n",
    "cnn_model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# Print CNN model summary\n",
    "cnn_model.summary()\n",
    "\n",
    "# Assuming X_train, y_train, X_test, y_test are defined previously\n",
    "\n",
    "# Train CNN model\n",
    "history_cnn = cnn_model.fit(X_train, y_train, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "# Evaluate CNN model on the test set\n",
    "test_loss_cnn, test_accuracy_cnn = cnn_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f'Test Loss (CNN): {test_loss_cnn:.4f}')\n",
    "print(f'Test Accuracy (CNN): {test_accuracy_cnn:.4f}')\n",
    "\n",
    "# Predictions on Test data for CNN model\n",
    "y_test_pred_cnn = cnn_model.predict(X_test)\n",
    "y_test_pred_classes_cnn = np.argmax(y_test_pred_cnn, axis=1)\n",
    "\n",
    "# Calculate additional metrics for CNN model\n",
    "f1_cnn = f1_score(y_test, y_test_pred_classes_cnn, average='weighted')\n",
    "precision_cnn = precision_score(y_test, y_test_pred_classes_cnn, average='weighted')\n",
    "recall_cnn = recall_score(y_test, y_test_pred_classes_cnn, average='weighted')\n",
    "print(f'F1 Score (CNN): {f1_cnn:.4f}')\n",
    "print(f'Precision (CNN): {precision_cnn:.4f}')\n",
    "print(f'Recall (CNN): {recall_cnn:.4f}')\n",
    "\n",
    "# Reuse model variable for LSTM model\n",
    "# Build LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_size, input_length=max_len))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile LSTM model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train LSTM model\n",
    "history_lstm = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "# Evaluate LSTM model on the test set\n",
    "test_loss_lstm, test_accuracy_lstm = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f'Test Loss (LSTM): {test_loss_lstm:.4f}')\n",
    "print(f'Test Accuracy (LSTM): {test_accuracy_lstm:.4f}')\n",
    "\n",
    "# Predictions on Test data for LSTM model\n",
    "y_test_pred_lstm = model.predict(X_test)\n",
    "y_test_pred_classes_lstm = np.argmax(y_test_pred_lstm, axis=1)\n",
    "\n",
    "# Calculate additional metrics for LSTM model\n",
    "f1_lstm = f1_score(y_test, y_test_pred_classes_lstm, average='weighted')\n",
    "precision_lstm = precision_score(y_test, y_test_pred_classes_lstm, average='weighted')\n",
    "recall_lstm = recall_score(y_test, y_test_pred_classes_lstm, average='weighted')\n",
    "print(f'F1 Score (LSTM): {f1_lstm:.4f}')\n",
    "print(f'Precision (LSTM): {precision_lstm:.4f}')\n",
    "print(f'Recall (LSTM): {recall_lstm:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46575924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on the test set\n",
    "loss, accuracy, precision, recall = model.evaluate(X_test, y_test, verbose=0)\n",
    "# Print metrics\n",
    "print('')\n",
    "print('Accuracy  : {:.4f}'.format(accuracy))\n",
    "print('Precision : {:.4f}'.format(precision))\n",
    "print('Recall    : {:.4f}'.format(recall))\n",
    "print('F1 Score  : {:.4f}'.format(f1_score(precision, recall)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_hist(history):\n",
    "    '''Function to plot history for accuracy and loss'''\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10,4))\n",
    "    # first plot\n",
    "    ax[0].plot(history.history['accuracy'])\n",
    "    ax[0].plot(history.history['val_accuracy'])\n",
    "    ax[0].set_title('Model Accuracy')\n",
    "    ax[0].set_xlabel('epoch')\n",
    "    ax[0].set_ylabel('accuracy')\n",
    "    ax[0].legend(['train', 'validation'], loc='best')\n",
    "    # second plot\n",
    "    ax[1].plot(history.history['loss'])\n",
    "    ax[1].plot(history.history['val_loss'])\n",
    "    ax[1].set_title('Model Loss')\n",
    "    ax[1].set_xlabel('epoch')\n",
    "    ax[1].set_ylabel('loss')\n",
    "    ax[1].legend(['train', 'validation'], loc='best')\n",
    "    \n",
    "plot_training_hist(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f9bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(model, X_test, y_test):\n",
    "    '''Function to plot confusion matrix for the passed model and the data'''\n",
    "    \n",
    "    sentiment_classes = ['Negative', 'Neutral', 'Positive']\n",
    "    # use model to do the prediction\n",
    "    y_pred = model.predict(X_test)\n",
    "    # compute confusion matrix\n",
    "    cm = confusion_matrix(np.argmax(np.array(y_test),axis=1), np.argmax(y_pred, axis=1))\n",
    "    # plot confusion matrix\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, cmap=plt.cm.Blues, annot=True, fmt='d', \n",
    "                xticklabels=sentiment_classes,\n",
    "                yticklabels=sentiment_classes)\n",
    "    plt.title('Confusion matrix', fontsize=16)\n",
    "    plt.xlabel('Actual label', fontsize=12)\n",
    "    plt.ylabel('Predicted label', fontsize=12)\n",
    "    \n",
    "plot_confusion_matrix(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e46d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model architecture & the weights\n",
    "model.save('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e53966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load model\n",
    "model = load_model('best_model.h5')\n",
    "\n",
    "def predict_class(text):\n",
    "    '''Function to predict sentiment class of the passed text'''\n",
    "    \n",
    "    sentiment_classes = ['Negative', 'Neutral', 'Positive']\n",
    "    max_len=50\n",
    "    \n",
    "    # Transforms text to a sequence of integers using a tokenizer object\n",
    "    xt = tokenizer.texts_to_sequences(text)\n",
    "    # Pad sequences to the same length\n",
    "    xt = pad_sequences(xt, padding='post', maxlen=max_len)\n",
    "    # Do the prediction using the loaded model\n",
    "    yt = model.predict(xt).argmax(axis=1)\n",
    "    # Print the predicted sentiment\n",
    "    print('The predicted sentiment is', sentiment_classes[yt[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
